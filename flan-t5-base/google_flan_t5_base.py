# -*- coding: utf-8 -*-
"""google/flan-t5-base.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Wt9dfOcAcccaf23osobEepKMPCQG9zb
"""

!pip install -q sentence-transformers faiss-cpu transformers accelerate datasets rapidfuzz safetensors
!pip install -q uvicorn fastapi nest-asyncio pydantic

!pip install transformers sentence-transformers faiss-cpu rapidfuzz

import pandas as pd
from sentence_transformers import SentenceTransformer
import faiss
from rapidfuzz import process, fuzz
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

df = pd.read_csv("ilaç_yeni.csv")


texts = (
    df['medicine_desc'].fillna('') + "\nYAN ETKİLER:\n" +
    df['side_effects'].fillna('') + "\nİLAÇ ETKİLEŞİMLERİ:\n" +
    df['drug_interactions'].fillna('')
).tolist()

metadatas = df.to_dict(orient='records')

embed_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

embeddings = embed_model.encode(texts, convert_to_numpy=True)

index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(embeddings)

def retrieve_drug_info(drug_name, top_k=3):
    q_emb = embed_model.encode([drug_name], convert_to_numpy=True)
    D, I = index.search(q_emb, top_k)
    results = []
    for idx in I[0]:
        results.append(texts[idx])
    return "\n".join(results)

model_id = "google/flan-t5-base"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to("cuda")

def classify_intent(q):
    q = q.lower()
    if any(w in q for w in ["yan etki", "zarar", "olumsuz"]):
        return "SIDE_EFFECT"
    if any(w in q for w in ["nasıl kullan", "doz", "talimat"]):
        return "USAGE"
    if any(w in q for w in ["birlikte", "etkileşim"]):
        return "INTERACTION"
    return "UNKNOWN"

product_names = [m['product_name'] for m in metadatas]
product_names_lower = [p.lower() for p in product_names]

def extract_drugs(question, top_n=3, score_cutoff=70):
    q = question.lower()
    found = []


    for name in product_names_lower:
        if name and name in q and name not in found:
            found.append(name)


    if not found:
        res = process.extract(q, product_names_lower, scorer=fuzz.token_sort_ratio, limit=top_n)
        for name, score, idx in res:
            if score >= score_cutoff:
                found.append(product_names_lower[idx])

    return found

def extract_relevant_sentences(text, intent):
    intent_keywords = {
        "SIDE_EFFECT": ["yan etki", "reaksiyon", "olumsuz", "tolerans"],
        "USAGE": ["kullan", "doz", "uygulama", "talimat"],
        "INTERACTION": ["etkileşim", "birlikte", "kombine"]
    }

    keywords = intent_keywords.get(intent, [])
    lines = [l.strip() for l in text.split("\n") if l.strip()]

    filtered = [
        l for l in lines
        if any(k in l.lower() for k in keywords)
    ]

    if not filtered:
        return lines[0]

    return "\n".join(filtered[:2])

def llm_answer(user_question, knowledge):

    chunks = [knowledge[i:i+400] for i in range(0, len(knowledge), 400)]
    answers = []
    for c in chunks:
        prompt = f"Aşağıdaki bilgilere dayanarak soruya kısa cevap ver:\n\n{c}\n\nSoru: {user_question}\nCevap:"
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(model.device)
        output = model.generate(**inputs, max_new_tokens=200)
        ans = tokenizer.decode(output[0], skip_special_tokens=True)
        answers.append(ans)
    return " ".join(answers)

def answer_question(q):
    intent = classify_intent(q)
    drugs = extract_drugs(q)

    if not drugs:
        return "Bu ilacı veri tabanında bulamadım."

    rag_text = retrieve_drug_info(drugs[0])
    full_llm = llm_answer(q, rag_text)
    clean = extract_relevant_sentences(full_llm, intent)

    return clean

print(" İlaç Bilgi Asistanı Hazır!")
print("Soru sor (Çıkmak için q yaz):\n")

while True:
    q = input("Soru: ")
    if q.lower() == "q":
        break
    print("\n--- Yanıt ---")
    print(answer_question(q))
    print("-------------\n")